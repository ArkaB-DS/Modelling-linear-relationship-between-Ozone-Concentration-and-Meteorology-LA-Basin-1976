---
title: "Ozone concentration and meteorology in the LA Basin,1976 - A Regression Study"
author:
- Arkajyoti Bhattacharjee
- Vishweshwar Tyagi
- Saurab Jain
- Apporva Singh
date: "19 April 2021"
output:
  beamer_presentation:
    colortheme: rose
    fonttheme: structurebold
    slide_level: 3
    theme: warsaw
  ioslides_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# Aim of the Study

The aforementioned dataset is a historical time-series data set. Our principal objective in this project would be to understand how the various meteorological variables affect the ozone concentration. We would first understand the regressors and see how they behave with respect to the response variable as well as with each other through basic exploratory data analysis. Further, we would deal with the violation of basic assumptions such as multicollinearity, auto-correlation, heteroscedasticity and normality of the errors as well as take necessary measures to rectify them. This will constitute a large chunk of feature engineering. Finally, we plan to carry out model selection across various models obtained in process while making use of adequate metrics.

# The Ozone Dataset    

We’ll make use of the ozone: __Ozone in LA in 1976^[1]__ dataset in order to study
the linear relationship between atmospheric ozone concentration and meteorology
in the Los Angeles Basin in 1976.

## Data description  

This dataset consists of 330 observations on the following 10 variables:

- __O3:__ _Ozone conc., ppm, at Sandbug AFB._

- __vh:__ _Vandenburg 500 millibar height (m)_

- __wind:__ _wind speed (mph)_

- __humidity:__ _a numeric vector_

- __temp:__ _temperature (C◦)_

- __ibh:__ _inversion base height (ft.)_

- __dpg:__ _Daggett pressure gradient (mmhg)_

- __ibt:__ _inversion base temperature (F◦)_

- __vis:__ _visibility (miles)_

- __doy:__ _day of the year_

Here, __O3__ is the response variable and the remaining 9 are the regressors, hence, this
is the case of __Multiple Linear Regression__.

### about the data  

# Explorartory Analysis  



### Model Assumptions

The model is given by :   
    $$O_3=\beta_0+\beta_1vh+\beta_2humidity+\beta_3wind+\beta_4temp+\beta_5dpg+\beta_6ibt+\beta_7ibh+\beta_8doy+\beta_9vis+\epsilon$$
  
  We assume a Gauss-Markov model i.e. we make the following assumptions:  
    1. $E(\epsilon)=0$  
    2. $var(\epsilon)=\sigma^2I$ i.e.   
      2.1. $var(\epsilon_i)=\sigma^2\,\,\forall\ i$  
      2.2. $cov(\epsilon_i,\epsilon_j)=0 \,\,\forall\,\, i\not=j$  
  
  In addition, for testing purposes, we assume   
    3. $\epsilon\sim N(0,\sigma^2I)$  

### The summary statistics , model plots and scatterplot matrix

```{r echo=FALSE}
# load data in R
#install.packages("faraway") # library where the data is
data(ozone,package="faraway") # loading the dataset in R
#attach(ozone)
```    

```{r echo=FALSE,fig.width=10,fig.height=10}
options(warn=-1)
# taking modulo 365 +1 
ozone<-data.frame(ozone[,-10],doy=ozone[,10]%%365+1)
ozone<-as.data.frame(rbind(ozone[307:330,],ozone[1:306,]))
str(ozone) # structure of the dataset
summary(ozone) # summary of the dataset
#install.packages("Hmisc") # library for plotting the histograms of all the variables
#library(Hmisc) # load the installed package
#par(mfrow=c(3,3))# create a 3x3 window for next 10 plots
#hist.data.frame(ozone,freq=FALSE) # plot the histograms
```

#### Linear Model and its result

```{r echo=FALSE}
lmod0<-lm(O3~.,data=ozone[1:300,]) # Model 0
par(mfrow=c(2,2)) # create a 2x2 window for next 4 plots
plot(lmod0) # 4 plots
 # summary of Model 0
```

Based on the above graphs, we observe the following -:   
    -There is curvature in the *residual vs fitted plot* indicating a non-linear relationship in the data-set.  
    - The *normal  Q-Q* plot shows fairly a straight line, indicating the errors are more-or-less normally distributed  
    - $17$, $53$ and $220^{th}$ row are potential outliers  

```{r echo=FALSE}
lmod0<-lm(O3~.,data=ozone[1:300,]) # Model 0
par(mfrow=c(2,2)) # create a 2x2 window for next 4 plots
summary(lmod0) # 4 plots
 # summary of Model 0
```

Based on the summary of the fitted model, we make the following observations -:  
    - The *Multiple R-squared* of the model is: *0.721* and the *Adjusted R-squared* is: *0.7096 *  
    - The absolute value of the estimate of the regression coefficient of *wind* is less than its standard error; it implies that we can drop that variable  
    - Since the errors seem to follow normal distribution based on *Q-Q* plot, so taking level of significance to be $0.01$, only *humidity* and *temperature* seem to be statistically significant based on their p-values.  

# Multicollinearity

- We first look at the __scatterplot matrix__ to get a better idea as to how the variables are related to each other
- Further we use __eigen-decomposition__ and __variance inflation factors (VIFs)__, to find out which regressors are responsible for multicollinearity.

### Scatterplot Matrix

```{r echo=FALSE,fig.width=10,fig.height=10}
pairs(O3~.,data=ozone[1:300,])
```

#### Observations of Scatterplot

  - *vh* and *temp* seem to be almost perfectly positively correlated  
  -   *temp* and *ibt* seem to be almost perfectly positively correlated  
  - As expected from the above two points, *vh* and *temp* seem to be almost perfectly positively correlated  

### Eigen-decompostion

```{r echo=FALSE}
options(warn=-1)
library(mctest)
eigprop(lmod0) # variance decompostion proportion
```



### Variance inflation factors(VIFs)

```{r echo=FALSE}
options(warn=-1)
#install.packages("car")
#library(car)
#vif(lmod0) # variance inflation factors 
```

Clearly, __vh, temp, humidity and vis__ have variance decomposition greater than 0.5 and VIFs>5

# Solving Multicolinearity Problem

We use three methods as a remedial measure. They are -:

- __Dropping Variables("lmodA")__
- __Ridge Regression("lmodB")__
- __Principal Components Regression__


## Variable Drop
- we drop the variables vh and ibt from the model and again fit the data
into a new model
- We compute the VIFs of lmodA and compute the R2 value the new model to see if there is any significant drop due to variables drop.

```{r echo=FALSE}
#vif(lm(O3~.-vh-ibt,data=ozone[1:300,])) #all <5
summary(lm(O3~.-vh-ibt,data=ozone[1:300,]))$r.squared
```

```{r echo=FALSE}
lmodA<-lm(O3~.-ibt-vh,data=ozone[1:300,])
``` 
Clearly, the VIFs are all less than 5 and apparently,the multicollinearity problem is solved.


## Ridge Regression
The Ridge complexity parameter is selected based on __Hoerl et al. 1975__ and it turns out to be K = 0.008.

The VIFs of our new model using ridge regression is as follow.

```{r echo=FALSE}
options(warn=-1)
library(lmridge)
``` 

```{r echo=FALSE}
lmodB<-lmridge(O3~vh+ibt+wind+humidity+temp+ibh+dpg+vis+doy,
data=ozone[1:300,],K=0.008)
#vif(lmodB)
```


## Principal Components Regression

We use all the PCs to fit a model.

```{r echo=FALSE}
pcr<-prcomp(ozone[1:300,-1],center=TRUE,scale=TRUE)
summary(pcr)
plot(pcr,type="l")
```
```{r echo=FALSE}
lmodC<-lm(ozone[1:300,1]~.,data=data.frame(pcr$x))
```


# Variable Selection



The __Occam Razor’s__ principle or the __law of parsimony__ state that the simplest explanation of an event or observation is the preferred explanation. Therefore, we need to do variable selection.

### Steps

- Plot the Akaike Information Criterion(AIC) against the number of regressors(p) and see for what p is the AIC minimum.
- Use “stepwise” or “exhaustive” method to find the best subset of regressors which has the minimum AIC.
- Apply this in all three methods discussed above.

## Model A

```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelA
b <- regsubsets(x=model.matrix(lmodA)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:8)*2
plot(AIC ~ I(1:7), ylab="AIC", xlab="Number of Predictors",
type="l")
```

- Based on the above plot, we see that for __5__ regressors, the __AIC__ is minimum. Also, corresponding to __5__, only __wind__ and __vis__ gets dropped from the current regressors' set.

- Finally we fit the model and found the $R^2$ is equal to __0.6913531__. 


##Model B

```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelB
b <- regsubsets(x=lmodB$xs,y=lmodB$y)
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```

- Based on the above plot, we see that for **4** regressors, the **AIC** is minimum. Also, corresponding to **4**, the regressors are **ibh**, **humidty**, **temp** and **vis**.

- The fitted model has $R^2$ value equal to __0.67850__ which is less than model A.

## Model C

```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelA
b <- regsubsets(x=model.matrix(lmodC)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```

- Based on the above plot, we see that for **5** principal components, the **AIC** is minimum. Also, corresponding to **5**, the principal components are 1, 4, 5, 7 and 8.

- The final fitted model has $R^2$ equal to __0.696608__ which is best among all.


# Heteroscedasticity of Errors

Use __Breusch-Pagan__ test to detect
heteroscedasticity and in case of its presence, we wil use __Box-Cox__ transformation as a remedy.

........ MAKE TABLE and show analysis for A,B and C..............

### Model A

- Firstly, we apply the bp test in which we found the value of __BP = 33.1__. Hence, it clearly indicate that the test gets rejected i.e. the errors are not homoscedastic based on the data.

- Now use the **box-cox** transform.

```{r echo=FALSE}
#install.packages("MASS")
library(MASS)
ans<-boxcox(lmodA)
lambdaA<-ans$x[which(ans$y==max(ans$y))]
lmodA<-lm(((O3^lambdaA-1)/lambdaA)~humidity+temp+ibh+dpg+doy,data=ozone[1:300,])
```

Finally, we see if the bp-test gets accepted and see the $R^2$ value of the new model.

```{r echo=FALSE}
library(lmtest)
summary(lmodA)$r.squared
bptest(lmodA) # accepted
```

Clearly, the test gets accepted and $R^2$ value is also significantly better.

### Model B


- We found that the test get rejected as same as above, here __BP = 30.654__

# Normality of Errors

-First see the __Q-Q plot__ of the residuals and then use the __Shapiro-Wilks__ test to confirm whether the errors follow normality or not.

### Model A

```{r echo=FALSE}
qqnorm(residuals(lmodA)) ## Q-Q Plot
qqline(residuals(lmodA))
shapiro.test(residuals(lmodA)) # accepted
```

__Shapiro-wilks__ test confirmed that the errors follow normality and also seen from graph.

### Model B

```{r echo=FALSE}
qqnorm(residuals(lmodB)) ## Q-Q Plot
qqline(residuals(lmodB))
shapiro.test(residuals(lmodB)) # accepted
```

It is evident from the graph that the errors follow normality and it is also confirmed by the Shapiro-wilks test.

### Model C

```{r echo=FALSE}
qqnorm(residuals(lmodC)) ## Q-Q Plot
qqline(residuals(lmodC))
shapiro.test(residuals(lmodC)) # accepted
```

Clearly, errors follow normality which is confirmed by Shapiro-wilks test and seen in graph.  

# Autocorrelation

- First look at the plots of $\epsilon_t\,\,vs. \,\,\epsilon_{t-1}$ for each model and see if there's a high correlation between them. 

- Secondly,use **Durbin-Watson** test to confirm the presence of autocorrelation.

### Model A

```{r echo=FALSE}
plot(residuals(lmodA)[-1],residuals(lmodA)[-length(residuals(lmodA))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodA)[-length(residuals(lmodA))]~
residuals(lmodA)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodA) # rejected
```

### Model B

```{r echo=FALSE}
plot(residuals(lmodB)[-1],residuals(lmodB)[-length(residuals(lmodB))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodB)[-length(residuals(lmodB))]~
residuals(lmodB)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodB) # rejected
```

### Model C

```{r echo=FALSE}
plot(residuals(lmodC)[-1],residuals(lmodC)[-length(residuals(lmodC))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodC)[-length(residuals(lmodC))]~
residuals(lmodC)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodC) # rejected
```


##### Observation

- All the models have auto-correlated residuals.

- Assuming __AR(p)__ model for the errors, we fitted models for $p=$1-20. None performed satisfactorily i.e. none achieved stationarity.

- Finally, we used the __auto.arima__ function in the forecast package in R that automatically fits an ARIMA(p,d,q) process by taking that value of d such that stationarity is achieved and p and q are chosen so that minimum AIC is achieved.

- In models **A** and **C**, an **ARIMA(0,1,2)** model is fitted. We do not take any remedial measure for model **B** as the problem then becomes too complicated.

