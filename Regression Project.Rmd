---
title: "Modelling-linear-relationship-between-Ozone-Concentration-and-Meteorology-LA-Basin-1976"
author: "Arkajyoti Bhattacharjee"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Acknowledgement
We would like to express our gratitude to *Dr.Sharmishtha Mitra* for giving the opportunity to do this project. It has been a great learning experience and has also provided us with a hands-on practical insight of the theoretical knowledge gathered during the course MTH416A: Regression Analysis.

\newpage

# Table of Contents

\newpage

# Introduction

  Although it represents only a tiny fraction of the atmosphere, ozone is crucial for life on Earth. With a weakening of this shield, we would be more susceptible to skin cancer, cataracts and impaired immune systems. Again. closer to Earth in the troposphere (the atmospheric layer from the surface up to about 10 km), ozone is a harmful pollutant that causes damage to lung tissue and plants.  
  
  In this project, we aim to understand the linear relationship between Ozone concentration and meteorological variables like temperature, pressure, humidity, etc. and develop parametric and non-parametric models to be able to predict ozone concentration based on given values of the meteorological variables.  
  
  We have fitted various regression models while detecting and taking remedial measures for the problems of multi-collinearity, hetroscedasticity and auto-correlation. After that, we compare the predictive power of the models developed in the process by compairing the Root Mean Square Error(RMSE) of the model.  
  
  The entire project is available in the Github link :     [*https://github.com/ArkaB-DS/Modelling-linear-relationship-between-Ozone-Concentration-and-Meteorology-LA-Basin-1976*](https://github.com/ArkaB-DS/Modelling-linear-relationship-between-Ozone-Concentration-and-Meteorology-LA-Basin-1976)  
  
\newpage

# About the Data

We will make use of the **Ozone in LA in 1976** dataset for this project. It is a historical time-series data.  

The variables associated with this dataset are as follows -    
  
  **O3:** Ozone conc., ppm, at Sandbug AFB.  
  **vh:** a numeric vector  
  **wind:** wind speed  
  **humidity:** a numeric vector  
  **temp:** temperature  
  **ibh:** inversion base height  
  **dpg:** Daggett pressure gradient  
  **ibt:** a numeric vector  
  **vis:** visibility  
  **doy:** day of the year  
  
  Here, **O3** is the response variable and the remaining are potential regressors.

## Source :
  
  *Breiman, L. and J. H. Friedman (1985). Estimating optimal transformations for multiple regression and correlation. Journal of the American Statistical Association 80, 580-598.*
  
## Link to the Data File :

  [*Ozone*](https://github.com/ArkaB-DS/Modelling-linear-relationship-between-Ozone-Concentration-and-Meteorology-LA-Basin-1976/blob/main/Ozone2.csv)

\newpage 

# Model Assumptions :  
  For a preliminary analysis, we fit a multiple linear regression model to the data, with **O3** as the response and all other variables as regressors.  
  
  The model is given by :   
    $$O_3=\beta_0+\beta_1vh+\beta_2humidity+\beta_3wind+\beta_4temp+\beta_5dpg+\beta_6ibt+\beta_7ibh+\beta_8doy+\beta_9vis+\epsilon$$
  
  We assume a Gauss-Markov model i.e. we make the following assumptions:  
    1. $E(\epsilon)=0$  
    2. $var(\epsilon)=\sigma^2I$ i.e.   
      2.1. $var(\epsilon_i)=\sigma^2\,\,\forall\ i$  
      2.2. $cov(\epsilon_i,\epsilon_j)=0 \,\,\forall\,\, i\not=j$  
  
  In addition, for testing purposes, we assume   
    3. $\epsilon\sim N(0,\sigma^2I)$  
    
# Preliminary Analysis - Data Structure, Summary and Exploratory Analysis:

We first load the data-set in **R**

```{r eval=FALSE}
# load data in R
install.packages("faraway") # library where the data is
data(ozone,package="faraway") # loading the dataset in R
```

```{r echo=FALSE}
# load data in R
#install.packages("faraway") # library where the data is
data(ozone,package="faraway") # loading the dataset in R
#attach(ozone)
```    

We look into the first 6 rows of the dataset to get an idea what values each variable is taking.

```{r eval=FALSE}
head(ozone)
```
```{r echo=FALSE}
head(ozone)
```

We take the **doy** variable and compute it modulo 365 and then add 1 to it to make it in the range 1-365. We then look into the structure of the data and compute basic summary statistics of the data. We plot the histograms of the variables as well.

```{r eval=FALSE}
# taking modulo 365 +1
ozone<-data.frame(ozone[,-10],"doy"=ozone[,10]%%365+1)
str(ozone) # structure of the dataset
summary(ozone) # summary of the dataset
install.packages("Hmisc") # library for plotting the histograms of all the variables
library(Hmisc) # load the installed package
par(mfrow=c(3,3))# create a 3x3 window for next 10 plots
hist.data.frame(ozone,freq=FALSE) # plot the histograms
```

```{r echo=FALSE,fig.width=10,fig.height=10}
options(warn=-1)
# taking modulo 365 +1 
ozone<-data.frame(ozone[,-10],doy=ozone[,10]%%365+1)
ozone<-as.data.frame(rbind(ozone[307:330,],ozone[1:306,]))
str(ozone) # structure of the dataset
summary(ozone) # summary of the dataset
#install.packages("Hmisc") # library for plotting the histograms of all the variables
library(Hmisc) # load the installed package
#par(mfrow=c(3,3))# create a 3x3 window for next 10 plots
hist.data.frame(ozone,freq=FALSE) # plot the histograms
```
As evident above, the data contains no NA values. [comment regarding the above pics]  

We divide the data into 80% for training and 20% for validation.    

Now, we fit a multiple linear regression model with *O3* as the response and all other variables as regressors. We plot the basic summary plots based on the fitted model as well to get more idea about the data.   

```{r eval=FALSE}
lmod0<-lm(O3~.,data=ozone[1:300,]) # Model 0
par(mfrow=c(2,2)) # create a 2x2 window for next 4 plots
plot(lmod0) # 4 plots
summary(lmod0) # summary of Model 0
```

```{r echo=FALSE}
lmod0<-lm(O3~.,data=ozone[1:300,]) # Model 0
par(mfrow=c(2,2)) # create a 2x2 window for next 4 plots
plot(lmod0) # 4 plots
summary(lmod0) # summary of Model 0
```

Based on the above graphs, we observe the following -   
    + There is curvature in the *residual vs fitted plot* indicating a non-linear relationship in the data-set.  
    + The *normal  Q-Q* plot shows fairly a straight line, indicating the errors are more-or-less normally distributed  
    + $17$, $53$ and $220^{th}$ row are potential outliers  
    + [add about the two other plots]    
  
Based on the summary of the fitted model, we make the following observations -  
    + The *Multiple R-squared* of the model is: *0.721* and the *Adjusted R-squared* is: *0.7096 *  
    + The absolute value of the estimate of the regression coefficient of *wind* is less than its standard error; it implies that we can drop that variable  
    + Since the errors seem to follow normal distribution based on *Q-Q* plot, so taking level of significance to be $0.01$, only *humidity* and *temperature* seem to be statistically significant based on their p-values.  
  
We now get into a deeper analysis of the data.  
    
\newpage

# Detection of Outliers and Influential Points 

\newpage

# Multicollinearity

  We first look at the *scatterplot matrix* to get a better idea as to how the variables are related to each other.  
  
```{r eval=FALSE}
pairs(O3~.,data=ozone[1:300,])
```
  
```{r echo=FALSE,fig.width=10,fig.height=10}
pairs(O3~.,data=ozone[1:300,])
```
  
 Based on the above *scatterplot matrix*, we make the following observations -      
    + *vh* and *temp* seem to be almost perfectly positively correlated  
    + *temp* and *ibt* seem to be almost perfectly positively correlated  
    + As expected from the above two points, *vh* and *temp* seem to be almost perfectly positively correlated  
  
Next, we use the eigen-decompostion proportion, to find out which regressors are responsible for multicollinearity.  

```{r eval=FALSE}
install.packages("mctest")
library(mctest)
eigprop(lmod0) # variance decompostion proportion
```

```{r echo=FALSE}
options(warn=-1)
library(mctest)
eigprop(lmod0) # variance decompostion proportion
```
  
 Clearly, *vh*,*temp*, *humidity*, *ibt* and *vis* have variance decompositon greater than 0.50. We, further, look into the variance inflation factors(VIFs) of the model for the same purpose.  

```{r eval=FALSE}
install.packages("car")
library(car)
vif(lmod0) # variance inflation factors 
```

```{r echo=FALSE}
options(warn=-1)
#install.packages("car")
library(car)
vif(lmod0) # variance inflation factors 
```
 Clearly, *vh*, *temp* and *ibt* have VIFs>5.    
 So, we have the problem of multicollinearity and we use three methods as a remedial measure -   
    1. Dropping Variables("lmodA")  
    2. Ridge Regression("lmodB")  
    3. Principal Components Regression  

## 1
 Now, based on the scatterplot matrix, we drop the variables *vh* and *ibt* from the model and again fit the data into a new model, say **lmodA**.  
 We compute the VIFs of **lmodA** and compute the $R^2$ value the new model to see if there is any significant drop due to variables drop.
 
```{r eval=FALSE}
vif(lm(O3~.-vh-ibt,data=ozone[1:300,])) #all <5
summary(lm(O3~.-vh-ibt,data=ozone[1:300,]))$r.squared
```

```{r echo=FALSE}
vif(lm(O3~.-vh-ibt,data=ozone[1:300,])) #all <5
summary(lm(O3~.-vh-ibt,data=ozone[1:300,]))$r.squared
```
 
 Recall that the $R^2$ value of **lmod0** is and that of **lmodA** is - not significantly lower from the former. Also, the VIFs are all less than 5 and apparently,the multicollinearity problem is solved. Hence, our new model is **lmodA**.  
 
```{r eval=FALSE}
lmodA<-lm(O3~.-ibt-vh,data=ozone[1:300,])
``` 

```{r echo=FALSE}
lmodA<-lm(O3~.-ibt-vh,data=ozone[1:300,])
``` 
 We ,again, look into the new scatterplot matrix, corresponding to **lmodA** to see how the remaining variables are inter-connected.  
 
```{r eval=FALSE}
 pairs(ozone[,c(1,3,4,5,6,7,9,10)])
```
 
```{r echo=FALSE,fig.height=10,fig.width=10}
 pairs(ozone[,c(1,3,4,5,6,7,9,10)])
```
   We make the following observations based on the above scatterplot matrix -   
      + There is a quadratic relationship between **temp** and **doy**. This is expected as temperature increases in the middle of the year and is lower elsewhere.    
      + A similar relationship exists between **dpg** and **doy** [intuitive justification?]    
      + [any thing else]    

## 2 
We employ ridge regression to solve the problem of multicollinearity.  
  
The Ridge complexity parameter is selected based on **Hoerl et al. 1975**.  

```{r eval=FALSE}
install.packages("lmridge")
library(lmridge)
lmodB<-lmridge(O3~vh+wind+humidity+temp+ibh+ibt+dpg+vis+doy,
data=ozone[1:300,],K=seq(0,0.1,1e-3)) # K=0.008
plot(lmodB)
```
```{r echo=FALSE}
options(warn=-1)
library(lmridge)
#lmodB<-lmridge(O3~vh+wind+humidity+temp+ibt+ibh+dpg+vis+doy,
#data=ozone[1:300,],K=seq(0,0.2,1e-4)) 
#plot(lmodB)
``` 
The Ridge complexity parameter turns out to be $K=0.008$.  

So, we define our Ridge regression model as **lmodB** and compute the summary of the model. We check out its VIFs as well.  

```{r eval=FALSE}
lmodB<-lmridge(O3~vh+ibt+wind+humidity+temp+ibh+dpg+vis+doy,
data=ozone[1:300,],K=0.008)
summary(lmodB)
vif(lmodB)
```
```{r echo=FALSE}
lmodB<-lmridge(O3~vh+ibt+wind+humidity+temp+ibh+dpg+vis+doy,
data=ozone[1:300,],K=0.008)
summary(lmodB)
vif(lmodB)
```
## 3

 Here, we use PCR to solve the problem of multi-collinearity.

```{r eval=FALSE}
pcr<-prcomp(ozone[1:300,-1],center=TRUE,scale=TRUE)
summary(pcr)
plot(pcr,type="l")
```

```{r echo=FALSE}
pcr<-prcomp(ozone[1:300,-1],center=TRUE,scale=TRUE)
summary(pcr)
plot(pcr,type="l")
```
We use all the PCs to fit a model. We will use variable selection later to select a smaller number of  PCs The summary of the fitted model, **lmodC** is below -   

```{r eval=FALSE}
lmodC<-lm(ozone[1:300,1]~.,data=data.frame(pcr$x))
summary(lmodC)$r.squared
```

```{r echo=FALSE}
lmodC<-lm(ozone[1:300,1]~.,data=data.frame(pcr$x))
summary(lmodC)$r.squared
```

\newpage

# Variable Selection

Now, because **Occam Razor**'s principle or the **law of parsimony**, we need to do variable selection.  

For this, we first plot the **Akaike Information Criterion(AIC)** against the **number of regressors(p)** and see for what **p** is the **AIC** minimum. We then use "stepwise" or "exhaustive" method to find the best subset of regressors which has the minimum AIC. We do this for each of the three models - **A**, **B** and **C** respectively.

## Model A

```{r eval=FALSE}
#install.packages("leaps")
library(leaps)
# for modelA
b <- regsubsets(x=model.matrix(lmodA)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:8)*2
plot(AIC ~ I(1:7), ylab="AIC", xlab="Number of Predictors",
type="l")
```

```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelA
b <- regsubsets(x=model.matrix(lmodA)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:8)*2
plot(AIC ~ I(1:7), ylab="AIC", xlab="Number of Predictors",
type="l")
```
Based on the above plot, we see that for **5** regressors, the **AIC** is minimum. Also, corresponding to **5**, only **wind** and **vis** gets dropped from the current regressors' set.

We also use stepwise regression to confirm this - 

```{r eval=FALSE}
step(lmodA)
```
```{r echo=FALSE}
step(lmodA)
```

Hence, the final fitted model is again named **lmodA** and its $R^2$ value is printed.

```{r eval=FALSE}
#O3~humidity+temp+ibh+vis+doy+dpg
lmodA<-lm(O3~humidity+temp+ibh+dpg+doy,data=ozone[c(1:300),])
summary(lmodA)$r.squared
```

```{r echo=FALSE}
#O3~humidity+temp+ibh+vis+doy+dpg
lmodA<-lm(O3~humidity+temp+ibh+dpg+doy,data=ozone[c(1:300),])
summary(lmodA)$r.squared
```
 The $R^2$ value is not significantly lower from **lmod0**.

## Model B

```{r eval=FALSE}
#install.packages("leaps")
library(leaps)
# for modelB
b <- regsubsets(x=lmodB$xs,y=lmodB$y)
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```

```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelB
b <- regsubsets(x=lmodB$xs,y=lmodB$y)
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```
Based on the above plot, we see that for **4** regressors, the **AIC** is minimum. Also, corresponding to **4**, the regressors are **ibh**, **humidty**, **temp** and **vis**.

Hence, the final fitted model is again named **lmodB** and its summary value is printed. here, we again need to find the ridge complexity parameter and using the iterative method, it turns out to be $K=0.018$

```{r eval=FALSE}
lmodB<-lmridge(O3~vh+ibt+humidity+temp+vis,
data=ozone[1:300,],K=seq(0,0.3,1e-3)) # K=0.018
plot(lmodB)
lmodB<-lmridge(O3~ibh+humidity+temp+vis,
data=ozone[1:300,],K=0.018) # K=0.013
summary(lmodB)
```
```{r echo=FALSE}
#lmodB<-lmridge(O3~vh+ibt+humidity+temp+vis,
#data=ozone[1:300,],K=seq(0,0.3,1e-3)) # K=0.011
#plot(lmodB)
lmodB<-lmridge(O3~humidity+temp+ibh+vis,
data=ozone[1:300,],K=0.018) # K=0.018
summary(lmodB)
```
 The $R^2$ value is now almost equal to that of  **lmod0**.

## Model C

```{r eval=FALSE}
#install.packages("leaps")
library(leaps)
# for modelC
b <- regsubsets(x=model.matrix(lmodC)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```
```{r echo=FALSE}
#install.packages("leaps")
library(leaps)
# for modelA
b <- regsubsets(x=model.matrix(lmodC)[,-1],y=ozone[1:300,1])
rs <- summary(b)
rs$which
AIC <- 300*log(rs$rss/300) + (2:9)*2
plot(AIC ~ I(1:8), ylab="AIC", xlab="Number of Predictors",
type="l")
```
Based on the above plot, we see that for **5** principal components, the **AIC** is minimum. Also, corresponding to **5**, the principal components are 1, 4, 5, 7 and 8.

We also use stepwise regression to confirm this - 

```{r eval=FALSE}
step(lmodC)
```
```{r echo=FALSE}
step(lmodC)
```

Hence, the final fitted model is again named **lmodC** and its $R^2$ value is printed.

```{r eval=FALSE}
#O3~humidity+temp+ibh+vis+doy+dpg
lmodC<-lm(ozone[1:300,]$O3~PC1+PC4+PC5+PC7+PC8,data=data.frame(pcr$x))
summary(lmodC)$r.squared
```

```{r echo=FALSE}
#O3~humidity+temp+ibh+vis+doy+dpg
lmodC<-lm(ozone[1:300,]$O3~PC1+PC4+PC5+PC7+PC8,data=data.frame(pcr$x))
summary(lmodC)$r.squared
```
 The $R^2$ value is not significantly lower from **lmod0**.

\newpage

# Heteroscedasticity of Errors

We now look into the homoscedasticity of errors assumption. We use **Breusch-Pagan** test to detect heteroscedasticity and in case of its presence, we wil use **Box-Cox** transformation as a remedy.

## Model A
```{r eval=FALSE}
install.packages("lmtest")
library(lmtest)
## for Model A
bptest(lmodA) # Breusch-Pagan test
```

```{r echo=FALSE}
options(warn=-1)
#install.packages("lmtest")
library(lmtest)
## for Model A
bptest(lmodA) # Breusch-Pagan test
```
As evident above, the test gets rejected i.e. the errors are not homoscedastic based on the data.

We now use the **box-cox** transform as follows -

```{r eval=FALSE}
install.packages("MASS")
library(MASS)
ans<-boxcox(lmodA)
lambdaA<-ans$x[which(ans$y==max(ans$y))]
lmodA<-lm(((O3^lambdaA-1)/lambdaA)~humidity+temp+ibh+dpg+doy,data=ozone[1:300,])
```

```{r echo=FALSE}
#install.packages("MASS")
library(MASS)
ans<-boxcox(lmodA)
lambdaA<-ans$x[which(ans$y==max(ans$y))]
lmodA<-lm(((O3^lambdaA-1)/lambdaA)~humidity+temp+ibh+dpg+doy,data=ozone[1:300,])
```
Finally, we see if the bp-test gets accepted and see the $R^2$ value of the new model, say **lmodA**, again.

```{r eval=FALSE}
summary(lmodA)$r.squared
bptest(lmodA) # accepted
```
```{r echo=FALSE}
summary(lmodA)$r.squared
bptest(lmodA) # accepted
```

Clearly, the test gets accepted and $R^2$ value is also significantly better.

## Model B
```{r eval=FALSE}
## for Model B
bptest(lmodB) # Breusch-Pagan test
```

```{r echo=FALSE}
bptest(lmodB) # Breusch-Pagan test
```
As evident above, the test gets rejected i.e. the errors are not homoscedastic based on the data.

We now use the **box-cox** transform as follows -

```{r eval=FALSE}
ans<-boxcox(lmodB)
lambdaB<-ans$x[which(ans$y==max(ans$y))]
lmodB<-lmridge(((O3^lambdaB-1)/lambdaB)~vh+ibt+humidity+temp+vis+ibh,
data=ozone[1:300,],K=0.014) # K=0.014
```

```{r echo=FALSE}
#ans<-boxcox(lmodB)
#lambdaB<-ans$x[which(ans$y==max(ans$y))]
lambdaB<-0.3
lmodB<-lmridge(((O3^lambdaB-1)/lambdaB)~vis+humidity+temp+ibh,
data=ozone[1:300,],K=0.007) # K=0.007
```
Finally, we see if the bp-test gets accepted and see the summary of the new model, say **lmodB**, again.

```{r eval=FALSE}
bptest(lmodB) # accepted
summary(lmodB)
```
```{r echo=FALSE}
bptest(lmodB) # accepted
summary(lmodB)
```

Clearly, the test gets accepted and $R^2$ value is also significantly better.

## Model C
```{r eval=FALSE}
## for Model B
bptest(lmodC) # Breusch-Pagan test
```

```{r echo=FALSE}
bptest(lmodC) # Breusch-Pagan test
```
As evident above, the test gets rejected i.e. the errors are not homoscedastic based on the data.

We now use the **box-cox** transform as follows -

```{r eval=FALSE}
ans<-boxcox(lmodC)
lambdaC<-ans$x[which(ans$y==max(ans$y))]
lmodC<-lm(((ozone[1:300,]$O3^lambdaC-1)/lambdaC)~PC1+PC4+PC5+PC7+PC8,data=data.frame(pcr$x))
```

```{r echo=FALSE}
ans<-boxcox(lmodC)
lambdaC<-ans$x[which(ans$y==max(ans$y))]
lmodC<-lm(((ozone[1:300,]$O3^lambdaC-1)/lambdaC)~PC1+PC4+PC5+PC7+PC8,data=data.frame(pcr$x))
```
Finally, we see if the bp-test gets accepted and see the $R^2$ value of the new model, say **lmodC**, again.

```{r eval=FALSE}
bptest(lmodC) # accepted
summary(lmodC)$r.squared
```
```{r echo=FALSE}
bptest(lmodC) # accepted
summary(lmodC)$r.squared
```

Clearly, the test gets accepted and $R^2$ value is also significantly better.

\newpage

# Normality of Errors

We first see the *Q-Q* plot of the residuals and then use the **Shapiro-Wilks** test to confirm whether the errors follow normality or not.

## A

```{r eval=FALSE}
qqnorm(residuals(lmodA)) ## Q-Q Plot
qqline(residuals(lmodA))
shapiro.test(residuals(lmodA)) # accepted
```
```{r echo=FALSE}
qqnorm(residuals(lmodA)) ## Q-Q Plot
qqline(residuals(lmodA))
shapiro.test(residuals(lmodA)) # accepted
```
It is evident from the graph that the errors follow normality and it is also confirmed by the Shapiro-wilks test.

## B

```{r eval=FALSE}
qqnorm(residuals(lmodB)) ## Q-Q Plot
qqline(residuals(lmodB))
shapiro.test(residuals(lmodB)) # accepted
```
```{r echo=FALSE}
qqnorm(residuals(lmodB)) ## Q-Q Plot
qqline(residuals(lmodB))
shapiro.test(residuals(lmodB)) # accepted
```
It is evident from the graph that the errors follow normality and it is also confirmed by the Shapiro-wilks test.

## C

```{r eval=FALSE}
qqnorm(residuals(lmodC)) ## Q-Q Plot
qqline(residuals(lmodC))
shapiro.test(residuals(lmodC)) # accepted
```
```{r echo=FALSE}
qqnorm(residuals(lmodC)) ## Q-Q Plot
qqline(residuals(lmodC))
shapiro.test(residuals(lmodC)) # accepted
```
It is evident from the graph that the errors follow normality and it is also confirmed by the Shapiro-wilks test.

\newpage

# Autocorrelation

We first look at the plots of $\epsilon_t\,\,vs. \,\,\epsilon_{t-1}$ and see if there's a high correlation between them. Then we will use 
**Durbin-Watson** test to confirm the presence of autocorrelation.

## Model A

```{r eval=FALSE}
plot(residuals(lmodA)[-1],residuals(lmodA)[-length(residuals(lmodA))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodA)[-length(residuals(lmodA))]~
residuals(lmodA)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodA) # rejected
```

```{r echo=FALSE}
plot(residuals(lmodA)[-1],residuals(lmodA)[-length(residuals(lmodA))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodA)[-length(residuals(lmodA))]~
residuals(lmodA)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodA) # rejected
```

## Model B

```{r eval=FALSE}
plot(residuals(lmodB)[-1],residuals(lmodB)[-length(residuals(lmodB))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodB)[-length(residuals(lmodB))]~
residuals(lmodB)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodB) # rejected
```

```{r echo=FALSE}
plot(residuals(lmodB)[-1],residuals(lmodB)[-length(residuals(lmodB))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodB)[-length(residuals(lmodB))]~
residuals(lmodB)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodB) # rejected
```
## Model C

```{r eval=FALSE}
plot(residuals(lmodC)[-1],residuals(lmodC)[-length(residuals(lmodC))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodC)[-length(residuals(lmodC))]~
residuals(lmodC)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodC) # rejected
```

```{r echo=FALSE}
plot(residuals(lmodC)[-1],residuals(lmodC)[-length(residuals(lmodC))],
xlab=expression(epsilon[t-1]),ylab=expression(epsilon[t]),
pch=13,col="red")
abline(lm(residuals(lmodC)[-length(residuals(lmodC))]~
residuals(lmodC)[-1]),lty=2,lwd=2,col="blue")
dwtest(lmodC) # rejected
```

All the models have auto-correlated residuals. Assuming **AR(p)** model for the errors, we fitted models for $p=$1-20. None performed satisfactorily i.e. none achieved  statioanrity.

Instead, we used the **auto.arima** function in the **forecast** package in **R** that automatically fits an **ARIMA(p,d,q)** process by taking that value of **d** such that stationarity is achieved and **p** and **q** are chosen so that minimum **AIC** is achieved.

In models **A** and **C**, an **ARIMA(0,1,2)** model is fitted. We do not take any remedial measure for model **B** as the problem then becomes too complicated.

```{r eval=FALSE}
library(forecast)
(modelA<-auto.arima(y=(ozone[c(1:300),1]^lambdaA-1)/lambdaA,
xreg=model.matrix(lmodA)[,-1],
max.p=7,max.q=7,max.d=7))
(modelC<-auto.arima(y=(ozone[c(1:300),1]^lambdaC-1)/lambdaC,
xreg=model.matrix(lmodC)[,-1],
max.p=7,max.q=7,max.d=7))
```

```{r echo=FALSE}
library(forecast)
(modelA<-auto.arima(y=(ozone[c(1:300),1]^lambdaA-1)/lambdaA,
xreg=model.matrix(lmodA)[,-1],
max.p=7,max.q=7,max.d=7))
(modelC<-auto.arima(y=(ozone[c(1:300),1]^lambdaC-1)/lambdaC,
xreg=model.matrix(lmodC)[,-1],
max.p=7,max.q=7,max.d=7))
```
 
 Now, we fit the final models as **modA** and **modC** and retain model B as **lmodB**
 
```{r eval=FALSE}
modA<-arima(x=(ozone[c(1:300),1]^lambdaA-1)/lambdaA,
xreg=model.matrix(lmodA)[,-1],
order=c(0,1,2))
modC<-arima(x=(ozone[c(1:300),1]^lambdaC-1)/lambdaC,
xreg=model.matrix(lmodC)[,-1],
order=c(0,1,2)) 
```

```{r echo=FALSE}
modA<-arima(x=(ozone[c(1:300),1]^lambdaA-1)/lambdaA,
xreg=model.matrix(lmodA)[,-1],
order=c(0,1,2))
modC<-arima(x=(ozone[c(1:300),1]^lambdaC-1)/lambdaC,
xreg=model.matrix(lmodC)[,-1],
order=c(0,1,2)) 
``` 
Possibly better models may be fitted after a course on *Time Series Analysis*.

\newpage

# Prediction

We finally predict using our models - **modA**, **lmodB** and **modC** with the test data,  i.e the last 20% of the ozone dataset.
We will use $RMSE=\sqrt{\frac{1}{n}\sum_i(y-\hat y_i)^2}$ as a metric to compare our models. The best of these three models will be the one with smaller. 

## Model A

```{r eval=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaA-1)/lambdaA
y_pred<-as.vector(predict(modA,
newxreg=ozone[301:330,c(4,5,6,7,10)])[[1]])
plot(y,type="o",col="red",ylim=c(0,5),lwd=2)
lines(y_pred,col="blue",type="o",lwd=2)
cat("The RMSE of model A is : ",sqrt(mean((y-y_pred)^2)))
```

```{r echo=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaA-1)/lambdaA
y_pred<-as.vector(predict(modA,
newxreg=ozone[301:330,c(4,5,6,7,10)])[[1]])
plot(y,type="o",col="red",ylim=c(0,5),lwd=2)
lines(y_pred,col="blue",type="o",lwd=2)
cat("The RMSE of model A is : ",sqrt(mean((y-y_pred)^2)))
```

## Model B

```{r eval=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaB-1)/lambdaB
y_pred<-predict(lmodB,ozone[301:330,-1],
type="response")
plot(y_pred,type="o",col="blue",ylim=c(0,5),lwd=2)
lines(y,col="red",type="o",lwd=2)
cat("The RMSE of model B is : ",sqrt(mean((y-y_pred)^2)))
```

```{r echo=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaB-1)/lambdaB
y_pred<-predict(lmodB,ozone[301:330,-1],
type="response")
plot(y_pred,type="o",col="blue",ylim=c(0,5),lwd=2)
lines(y,col="red",type="o",lwd=2)
cat("The RMSE of model B is : ",sqrt(mean((y-y_pred)^2)))
```

## Model C

```{r eval=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaC-1)/lambdaC
pcr<-prcomp(ozone[301:330,-1],center=TRUE,scale=TRUE)
Data<-data.frame(pcr$x)[,-c(2,3,6,9)]
#par(mfrow=c(1,2))
y_pred<-as.vector(predict(modC,newxreg=Data)[[1]])
plot(y,type="o",col="red",ylim=c(0,6),lwd=2)
lines(y_pred,col="blue",type="o",lwd=2)
cat("The RMSE of Model C is: ",sqrt(mean((y-y_pred)^2)))
```

```{r echo=FALSE}
y<-ozone[301:330,1]
y<-(y^lambdaC-1)/lambdaC
pcr<-prcomp(ozone[301:330,-1],center=TRUE,scale=TRUE)
Data<-data.frame(pcr$x)[,-c(2,3,6,9)]
#par(mfrow=c(1,2))
y_pred<-as.vector(predict(modC,newxreg=Data)[[1]])
plot(y,type="o",col="red",ylim=c(0,6),lwd=2)
lines(y_pred,col="blue",type="o",lwd=2)
cat("The RMSE of Model C is: ",sqrt(mean((y-y_pred)^2)))
```

So, based on the RMSE values, model **B** performs best, even without auto-correlation correction, with model **A** being a close competitor. Model **C** performs poorly, both evident from the graph.

\newpage

# Alternating Conditional Expectation(ACE) 
  We apply the **ACE** algorithm on the ozone dataset and see how it performs in terms of RMSE.
  
[Describe the ACE algorithm a bit]  

We first store the transformed variables  use **ACE** algorithm
```{r eval=FALSE}
library(acepack) # for the ACE algorithm
final<-ace(x=as.matrix(ozone[1:300,-1]),
y=ozone[1:300,1]) # storing the optimal transformed variables
Data<-data.frame(O3=final$ty,final$tx)
```

```{r echo=FALSE}
data(ozone,package="faraway") # loading the dataset in R
# changing the variable doy to day and taking modulo 365 
ozone<-data.frame(ozone[,-10],"day"=ozone[,10]%%365+1)
ozone<-as.data.frame(rbind(ozone[307:330,],ozone[1:306,]))
library(acepack) # for the ACE algorithm
final<-ace(x=as.matrix(ozone[1:300,-1]),
y=ozone[1:300,1]) # storing the optimal transformed variables
Data<-data.frame(O3=final$ty,final$tx)

```
We look at the resulting transformations plotting the transformed vs original variables.

```{r eval=FALSE}
par(mfrow=c(2,5))
for (i in 1:10) plot(ozone[1:300,i],
Data[,i],col=i,xlab="Original",ylab="Optimal Transform")
```

```{r echo=FALSE}
par(mfrow=c(2,5))
for (i in 1:10) plot(ozone[1:300,i],
Data[,i],col=i,main="Optimal Transform vs. Original")
```
Next, we fit a linear model to the optimal transformed dataset and look into the summary of the fitted model **lmod**.

```{r eval=FALSE}
lmod<-lm(O3~.,data=Data) # fitting a model after optimal transformations
summary(lmod)$r.squared
```

```{r echo=FALSE}
lmod<-lm(O3~.,data=Data) # fitting a model after optimal transformations
summary(lmod)$r.squared
```
Clearly, the $R^2$ value is quite high compared to our previous models.

Now, we finally predict using **lmod** and compute its RMSE.

```{r eval=FALSE}
final2 <- ace(x=as.matrix(ozone[301:330,-1]),y=ozone[301:330,1]) # test data
New <- data.frame(final2$tx) 
y_pred<-as.vector(predict(lmod,newdata=New,type="response")) # predicted values
plot(final2$ty,type="o",col="red",ylim=c(-2,2.5),lwd=2) # original vs predicted
lines(y_pred,type="o",col="blue",lwd=2)
cat("The RMSE  value of lmod is: ",sqrt(mean((final2$ty-y_pred)^2)))
```
```{r echo=FALSE}
final2 <- ace(x=as.matrix(ozone[301:330,-1]),y=ozone[301:330,1]) # test data
New <- data.frame(final2$tx) 
y_pred<-as.vector(predict(lmod,newdata=New,type="response")) # predicted values
plot(final2$ty,type="o",col="red",ylim=c(-2,2.5),lwd=2) # original vs predicted
lines(y_pred,type="o",col="blue",lwd=2)
cat("The RMSE  value of lmod is: ",sqrt(mean((final2$ty-y_pred)^2)))
```
The RMSE value is 0.45 - quite remarkable on comparison with **modA**, **lmodB** and **modC**.

Based on previous experience, we know that **ibt** and **temp** are almost perfectly correlated and **vh** showed a similar relationship with either of them.

We again fit a linear model, **final**, based on the transformed data, removing **ibt** and **vh**. 

```{r eval=FALSE}
final<-ace(x=as.matrix(ozone[1:300,-c(1,2,8)]),
y=ozone[1:300,1]) # storing the optimal transformed variables
Data<-data.frame(O3=final$ty,final$tx)
lmod<-lm(O3~.,data=Data) # fitting a model after optimal transformations
cat("The R-squared value of the final model is: ",summary(lmod)$r.squared)
final2 <- ace(x=as.matrix(ozone[301:330,-c(1,2,8)]),
y=ozone[301:330,1]) # test data
New <- data.frame(final2$tx) 
y_pred<-as.vector(predict(lmod,newdata=New,type="response")) # predicted values
plot(final2$ty,type="o",col="red",ylim=c(-2,2.5),lwd=2) # original vs predicted
lines(y_pred,type="o",col="blue",lwd=2)
cat("The RMSE  value of final is: ",sqrt(mean((final2$ty-y_pred)^2)))
```

```{r echo=FALSE}
final<-ace(x=as.matrix(ozone[1:300,-c(1,2,8)]),
y=ozone[1:300,1]) # storing the optimal transformed variables
Data<-data.frame(O3=final$ty,final$tx)
lmod<-lm(O3~.,data=Data) # fitting a model after optimal transformations
cat("The R-squared value of the final model is: ",summary(lmod)$r.squared)
final2 <- ace(x=as.matrix(ozone[301:330,-c(1,2,8)]),
y=ozone[301:330,1]) # test data
New <- data.frame(final2$tx) 
y_pred<-as.vector(predict(lmod,newdata=New,type="response")) # predicted values
plot(final2$ty,type="o",col="red",ylim=c(-2,2.5),lwd=2) # original vs predicted
lines(y_pred,type="o",col="blue",lwd=2)
cat("The RMSE  value of lmod is: ",sqrt(mean((final2$ty-y_pred)^2)))
```
The $R^2$ value of the **final** model is 0.82 and the RMSE value of the model  is 0.31 - both significantly better than our previous models.

As a final check, we see if our *final** model has any problem of **multicollinearity**, **heteroscedasticity** of errors, **non-normality** of errors and **auto-coorelation** of errors. 

```{r eval=FALSE}
vif(lmod)
shapiro.test(residuals(lmod)) # accepted
bptest(lmod)
dwtest(lmod,alternative="two.sided")
```
```{r echo=FALSE}
detach("package:lmridge",character.only=TRUE)
vif(lmod)
shapiro.test(residuals(lmod)) # accepted
bptest(lmod)
dwtest(lmod,alternative="two.sided")
```
As evident from the above tests, the **final** model accepts all tests and seems to be an ideal model compared to the previous models.